{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47210983",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61350ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG19, MobileNetV2\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f299bb7d",
   "metadata": {},
   "source": [
    "### Set Dataset Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9320ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"E:\\A\\OneDrive\\Documents\\3rd year\\5th SEM Materials of SR.Mishra(3rd yr)\\Deep Learning for Image Analytics\\Project\\Groundnut_Leaf_dataset\\Main Used Dataset\"\n",
    "\n",
    "img_size = (128, 128)     # smaller = faster\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1c666",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37a4536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6329 images belonging to 6 classes.\n",
      "Found 1581 images belonging to 6 classes.\n",
      "Number of classes: 6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Dataset path\n",
    "train_dir = r\"E:\\A\\OneDrive\\Documents\\3rd year\\5th SEM Materials of SR.Mishra(3rd yr)\\Deep Learning for Image Analytics\\Project\\Groundnut_Leaf_dataset\\Main Used Dataset\"\n",
    "\n",
    "# Image parameters\n",
    "img_size = (128, 128)\n",
    "batch_size = 32\n",
    "\n",
    "# Data generator with rescaling and validation split\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,      # Normalize pixel values to [0,1]\n",
    "    validation_split=0.2 # 20% images for validation\n",
    ")\n",
    "\n",
    "# Training generator\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(train_gen.class_indices)\n",
    "\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61d207",
   "metadata": {},
   "source": [
    "### Helper Function for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90358b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model, preprocess_func):\n",
    "    base_model.trainable = False\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(128,128,3)),\n",
    "        tf.keras.layers.Lambda(preprocess_func),\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(1e-4),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aeeea1",
   "metadata": {},
   "source": [
    "### Train with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ae8e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 818ms/step - accuracy: 0.5573 - loss: 1.2236 - val_accuracy: 0.6743 - val_loss: 0.9623\n",
      "Epoch 2/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 377ms/step - accuracy: 0.7589 - loss: 0.6782 - val_accuracy: 0.7211 - val_loss: 0.8225\n",
      "Epoch 3/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 581ms/step - accuracy: 0.8173 - loss: 0.5251 - val_accuracy: 0.7445 - val_loss: 0.7752\n",
      "Epoch 4/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 659ms/step - accuracy: 0.8513 - loss: 0.4258 - val_accuracy: 0.7672 - val_loss: 0.7361\n",
      "Epoch 5/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 508ms/step - accuracy: 0.8757 - loss: 0.3617 - val_accuracy: 0.7818 - val_loss: 0.7036\n",
      "Epoch 6/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 653ms/step - accuracy: 0.8886 - loss: 0.3212 - val_accuracy: 0.7862 - val_loss: 0.7214\n",
      "Epoch 7/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 655ms/step - accuracy: 0.9085 - loss: 0.2751 - val_accuracy: 0.7963 - val_loss: 0.6698\n",
      "Epoch 8/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 656ms/step - accuracy: 0.9182 - loss: 0.2494 - val_accuracy: 0.8121 - val_loss: 0.6825\n",
      "Epoch 9/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 621ms/step - accuracy: 0.9272 - loss: 0.2252 - val_accuracy: 0.8109 - val_loss: 0.6543\n",
      "Epoch 10/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 367ms/step - accuracy: 0.9344 - loss: 0.2040 - val_accuracy: 0.8178 - val_loss: 0.6652\n",
      "Epoch 11/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 366ms/step - accuracy: 0.9450 - loss: 0.1863 - val_accuracy: 0.8242 - val_loss: 0.6661\n",
      "Epoch 12/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 948ms/step - accuracy: 0.9442 - loss: 0.1723 - val_accuracy: 0.8324 - val_loss: 0.6576\n",
      "Epoch 13/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 380ms/step - accuracy: 0.9507 - loss: 0.1549 - val_accuracy: 0.8242 - val_loss: 0.6909\n",
      "Epoch 14/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 377ms/step - accuracy: 0.9561 - loss: 0.1416 - val_accuracy: 0.8330 - val_loss: 0.6668\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',       # monitor validation loss\n",
    "    patience=5,               # stop if no improvement for 5 epochs\n",
    "    restore_best_weights=True # keep best model weights\n",
    ")\n",
    "\n",
    "# Build ResNet50 base model\n",
    "resnet_base = ResNet50(\n",
    "    weights=\"imagenet\",       # use pretrained ImageNet weights\n",
    "    include_top=False,        # exclude the fully connected layer\n",
    "    input_shape=(128, 128, 3)\n",
    ")\n",
    "\n",
    "# Build full model using helper function\n",
    "resnet_model = build_model(resnet_base, resnet_preprocess)\n",
    "\n",
    "# Train the model\n",
    "history_resnet = resnet_model.fit(\n",
    "    train_gen,                # training generator\n",
    "    validation_data=val_gen,  # validation generator\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57661e6c",
   "metadata": {},
   "source": [
    "### Train with VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d240b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 3s/step - accuracy: 0.2914 - loss: 4.9793 - val_accuracy: 0.4118 - val_loss: 2.1435\n",
      "Epoch 2/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m554s\u001b[0m 3s/step - accuracy: 0.4582 - loss: 2.1387 - val_accuracy: 0.5161 - val_loss: 1.5004\n",
      "Epoch 3/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 3s/step - accuracy: 0.5280 - loss: 1.4845 - val_accuracy: 0.5674 - val_loss: 1.2739\n",
      "Epoch 4/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 3s/step - accuracy: 0.5841 - loss: 1.1848 - val_accuracy: 0.6015 - val_loss: 1.1410\n",
      "Epoch 5/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 3s/step - accuracy: 0.6285 - loss: 1.0285 - val_accuracy: 0.6281 - val_loss: 1.0965\n",
      "Epoch 6/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 3s/step - accuracy: 0.6571 - loss: 0.9265 - val_accuracy: 0.6490 - val_loss: 1.0429\n",
      "Epoch 7/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 3s/step - accuracy: 0.6939 - loss: 0.8285 - val_accuracy: 0.6502 - val_loss: 1.0255\n",
      "Epoch 8/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1659s\u001b[0m 8s/step - accuracy: 0.7175 - loss: 0.7592 - val_accuracy: 0.6660 - val_loss: 0.9935\n",
      "Epoch 9/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 3s/step - accuracy: 0.7293 - loss: 0.7169 - val_accuracy: 0.6818 - val_loss: 0.9521\n",
      "Epoch 10/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m445s\u001b[0m 2s/step - accuracy: 0.7483 - loss: 0.6691 - val_accuracy: 0.6875 - val_loss: 0.9554\n",
      "Epoch 11/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 2s/step - accuracy: 0.7752 - loss: 0.6082 - val_accuracy: 0.6875 - val_loss: 0.9616\n",
      "Epoch 12/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 2s/step - accuracy: 0.7816 - loss: 0.5768 - val_accuracy: 0.7008 - val_loss: 0.9371\n",
      "Epoch 13/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 2s/step - accuracy: 0.7924 - loss: 0.5472 - val_accuracy: 0.7034 - val_loss: 0.9406\n",
      "Epoch 14/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 2s/step - accuracy: 0.8104 - loss: 0.5225 - val_accuracy: 0.7034 - val_loss: 0.9418\n",
      "Epoch 15/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 2s/step - accuracy: 0.8132 - loss: 0.5062 - val_accuracy: 0.7097 - val_loss: 0.9208\n",
      "Epoch 16/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.8208 - loss: 0.4738 - val_accuracy: 0.7173 - val_loss: 0.9063\n",
      "Epoch 17/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m816s\u001b[0m 4s/step - accuracy: 0.8328 - loss: 0.4422 - val_accuracy: 0.7274 - val_loss: 0.9173\n",
      "Epoch 18/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m641s\u001b[0m 3s/step - accuracy: 0.8428 - loss: 0.4232 - val_accuracy: 0.7268 - val_loss: 0.9130\n",
      "Epoch 19/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 3s/step - accuracy: 0.8448 - loss: 0.4102 - val_accuracy: 0.7318 - val_loss: 0.9202\n",
      "Epoch 20/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 3s/step - accuracy: 0.8527 - loss: 0.3920 - val_accuracy: 0.7388 - val_loss: 0.9015\n",
      "Epoch 21/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 2s/step - accuracy: 0.8651 - loss: 0.3685 - val_accuracy: 0.7451 - val_loss: 0.9241\n",
      "Epoch 22/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m595s\u001b[0m 3s/step - accuracy: 0.8695 - loss: 0.3478 - val_accuracy: 0.7464 - val_loss: 0.9034\n",
      "Epoch 23/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3921s\u001b[0m 20s/step - accuracy: 0.8745 - loss: 0.3440 - val_accuracy: 0.7584 - val_loss: 0.8834\n",
      "Epoch 24/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 2s/step - accuracy: 0.8812 - loss: 0.3196 - val_accuracy: 0.7489 - val_loss: 0.9341\n",
      "Epoch 25/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 2s/step - accuracy: 0.8842 - loss: 0.3150 - val_accuracy: 0.7584 - val_loss: 0.9103\n",
      "Epoch 26/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 2s/step - accuracy: 0.8877 - loss: 0.2976 - val_accuracy: 0.7540 - val_loss: 0.9175\n",
      "Epoch 27/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 2s/step - accuracy: 0.8892 - loss: 0.2851 - val_accuracy: 0.7571 - val_loss: 0.9409\n",
      "Epoch 28/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 2s/step - accuracy: 0.8935 - loss: 0.2827 - val_accuracy: 0.7565 - val_loss: 0.9494\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',       # monitor validation loss\n",
    "    patience=5,               # stop if no improvement for 5 epochs\n",
    "    restore_best_weights=True # keep best model weights\n",
    ")\n",
    "\n",
    "# Build VGG19 base model\n",
    "vgg_base = VGG19(\n",
    "    weights=\"imagenet\",       # use pretrained ImageNet weights\n",
    "    include_top=False,        # exclude the fully connected layer\n",
    "    input_shape=(128, 128, 3)\n",
    ")\n",
    "\n",
    "# Build full model using helper function\n",
    "vgg_model = build_model(vgg_base, vgg_preprocess)\n",
    "\n",
    "# Train the model\n",
    "history_vgg = vgg_model.fit(\n",
    "    train_gen,                # training generator\n",
    "    validation_data=val_gen,  # validation generator\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3fefa",
   "metadata": {},
   "source": [
    "\n",
    "### Train with MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe6f80c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 181ms/step - accuracy: 0.5072 - loss: 1.2919 - val_accuracy: 0.6559 - val_loss: 1.0056\n",
      "Epoch 2/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 173ms/step - accuracy: 0.7387 - loss: 0.7341 - val_accuracy: 0.7394 - val_loss: 0.8034\n",
      "Epoch 3/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 187ms/step - accuracy: 0.8006 - loss: 0.5723 - val_accuracy: 0.7660 - val_loss: 0.7312\n",
      "Epoch 4/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 191ms/step - accuracy: 0.8407 - loss: 0.4673 - val_accuracy: 0.7799 - val_loss: 0.6878\n",
      "Epoch 5/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 186ms/step - accuracy: 0.8561 - loss: 0.4075 - val_accuracy: 0.7970 - val_loss: 0.6600\n",
      "Epoch 6/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 191ms/step - accuracy: 0.8772 - loss: 0.3594 - val_accuracy: 0.7951 - val_loss: 0.6601\n",
      "Epoch 7/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6679s\u001b[0m 34s/step - accuracy: 0.8915 - loss: 0.3154 - val_accuracy: 0.8052 - val_loss: 0.6505\n",
      "Epoch 8/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 184ms/step - accuracy: 0.8987 - loss: 0.2913 - val_accuracy: 0.8058 - val_loss: 0.6485\n",
      "Epoch 9/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 193ms/step - accuracy: 0.9077 - loss: 0.2711 - val_accuracy: 0.8115 - val_loss: 0.6422\n",
      "Epoch 10/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 193ms/step - accuracy: 0.9145 - loss: 0.2451 - val_accuracy: 0.8172 - val_loss: 0.6352\n",
      "Epoch 11/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 195ms/step - accuracy: 0.9207 - loss: 0.2279 - val_accuracy: 0.8191 - val_loss: 0.6404\n",
      "Epoch 12/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 192ms/step - accuracy: 0.9284 - loss: 0.2138 - val_accuracy: 0.8197 - val_loss: 0.6481\n",
      "Epoch 13/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 194ms/step - accuracy: 0.9370 - loss: 0.1975 - val_accuracy: 0.8261 - val_loss: 0.6401\n",
      "Epoch 14/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 189ms/step - accuracy: 0.9360 - loss: 0.1844 - val_accuracy: 0.8235 - val_loss: 0.6432\n",
      "Epoch 15/50\n",
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 188ms/step - accuracy: 0.9433 - loss: 0.1680 - val_accuracy: 0.8267 - val_loss: 0.6421\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',       # monitor validation loss\n",
    "    patience=5,               # stop if no improvement for 5 epochs\n",
    "    restore_best_weights=True # keep best model weights\n",
    ")\n",
    "\n",
    "# Build MobileNetV2 base model\n",
    "mobilenet_base = MobileNetV2(\n",
    "    weights=\"imagenet\",       # use pretrained ImageNet weights\n",
    "    include_top=False,        # exclude the fully connected layer\n",
    "    input_shape=(128, 128, 3)\n",
    ")\n",
    "\n",
    "# Build full model using helper function\n",
    "mobilenet_model = build_model(mobilenet_base, mobilenet_preprocess)\n",
    "\n",
    "# Train the model\n",
    "history_mobilenet = mobilenet_model.fit(\n",
    "    train_gen,                # training generator\n",
    "    validation_data=val_gen,  # validation generator\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13aa2f",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07519171",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must call `compile()` before using the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ResNet50 Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mresnet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m VGG19 Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, vgg_model\u001b[38;5;241m.\u001b[39mevaluate(val_gen, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m MobileNetV2 Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mobilenet_model\u001b[38;5;241m.\u001b[39mevaluate(val_gen, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:1050\u001b[0m, in \u001b[0;36mTrainer._assert_compile_called\u001b[1;34m(self, method_name)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1050\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You must call `compile()` before using the model."
     ]
    }
   ],
   "source": [
    "# Evaluate all models on validation set\n",
    "resnet_acc = resnet_model.evaluate(val_gen, verbose=0)[1]\n",
    "vgg_acc = vgg_model.evaluate(val_gen, verbose=0)[1]\n",
    "mobilenet_acc = mobilenet_model.evaluate(val_gen, verbose=0)[1]\n",
    "\n",
    "print(f\"ResNet50 Accuracy: {resnet_acc:.4f}\")\n",
    "print(f\"VGG19 Accuracy: {vgg_acc:.4f}\")\n",
    "print(f\"MobileNetV2 Accuracy: {mobilenet_acc:.4f}\")\n",
    "\n",
    "# Automatically determine best model\n",
    "accuracies = {\n",
    "    \"ResNet50\": resnet_acc,\n",
    "    \"VGG19\": vgg_acc,\n",
    "    \"MobileNetV2\": mobilenet_acc\n",
    "}\n",
    "best_model_name = max(accuracies, key=accuracies.get)\n",
    "print(\"✅ Best Model:\", best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff14748",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# ResNet50\n",
    "plt.plot(history_resnet.history['accuracy'], '--', label=\"ResNet50 Train\")\n",
    "plt.plot(history_resnet.history['val_accuracy'], label=\"ResNet50 Val\")\n",
    "\n",
    "# VGG19\n",
    "plt.plot(history_vgg.history['accuracy'], '--', label=\"VGG19 Train\")\n",
    "plt.plot(history_vgg.history['val_accuracy'], label=\"VGG19 Val\")\n",
    "\n",
    "# MobileNetV2\n",
    "plt.plot(history_mobilenet.history['accuracy'], '--', label=\"MobileNetV2 Train\")\n",
    "plt.plot(history_mobilenet.history['val_accuracy'], label=\"MobileNetV2 Val\")\n",
    "\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156242d",
   "metadata": {},
   "source": [
    "### Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically save the best model\n",
    "if best_model_name == \"ResNet50\":\n",
    "    resnet_model.save(\"best_groundnut_model.h5\")\n",
    "elif best_model_name == \"VGG19\":\n",
    "    vgg_model.save(\"best_groundnut_model.h5\")\n",
    "else:\n",
    "    mobilenet_model.save(\"best_groundnut_model.h5\")\n",
    "\n",
    "print(\"✅ Best model saved as 'best_groundnut_model.h5'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
